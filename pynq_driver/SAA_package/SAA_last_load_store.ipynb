{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAA_top测试\n",
    "## 1. 加载Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "[[  0   1   2   3   4   5   6   7   8   9  10]\n",
      " [ 11  12  13  14  15  16  17  18  19  20  21]\n",
      " [ 22  23  24  25  26  27  28  29  30  31  32]\n",
      " [ 33  34  35  36  37  38  39  40  41  42  43]\n",
      " [ 44  45  46  47  48  49  50  51  52  53  54]\n",
      " [ 55  56  57  58  59  60  61  62  63  64  65]\n",
      " [ 66  67  68  69  70  71  72  73  74  75  76]\n",
      " [ 77  78  79  80  81  82  83  84  85  86  87]\n",
      " [ 88  89  90  91  92  93  94  95  96  97  98]\n",
      " [ 99 100 101 102 103 104 105 106 107 108 109]\n",
      " [110 111 112 113 114 115 116 117 118 119 120]]\n",
      "Padded matrix:\n",
      "[[  0   1   2   3   4   5   6   7   8   9  10   0]\n",
      " [ 11  12  13  14  15  16  17  18  19  20  21   0]\n",
      " [ 22  23  24  25  26  27  28  29  30  31  32   0]\n",
      " [ 33  34  35  36  37  38  39  40  41  42  43   0]\n",
      " [ 44  45  46  47  48  49  50  51  52  53  54   0]\n",
      " [ 55  56  57  58  59  60  61  62  63  64  65   0]\n",
      " [ 66  67  68  69  70  71  72  73  74  75  76   0]\n",
      " [ 77  78  79  80  81  82  83  84  85  86  87   0]\n",
      " [ 88  89  90  91  92  93  94  95  96  97  98   0]\n",
      " [ 99 100 101 102 103 104 105 106 107 108 109   0]\n",
      " [110 111 112 113 114 115 116 117 118 119 120   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "Buffer content (flattened):\n",
      "[  0   1   2   3  11  12  13  14  22  23  24  25  33  34  35  36   4   5\n",
      "   6   7  15  16  17  18  26  27  28  29  37  38  39  40   8   9  10   0\n",
      "  19  20  21   0  30  31  32   0  41  42  43   0  44  45  46  47  55  56\n",
      "  57  58  66  67  68  69  77  78  79  80  48  49  50  51  59  60  61  62\n",
      "  70  71  72  73  81  82  83  84  52  53  54   0  63  64  65   0  74  75\n",
      "  76   0  85  86  87   0  88  89  90  91  99 100 101 102 110 111 112 113\n",
      "   0   0   0   0  92  93  94  95 103 104 105 106 114 115 116 117   0   0\n",
      "   0   0  96  97  98   0 107 108 109   0 118 119 120   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saa Overlay downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pynq import allocate\n",
    "import random\n",
    "import time\n",
    "from saa_insn_driver_last import * \n",
    "from saa_utils import * \n",
    "from pynq import Overlay\n",
    "\n",
    "# 加载Overlay\n",
    "overlay = Overlay(\"saa1.bit\")\n",
    "print(\"saa Overlay downloaded successfully!\")\n",
    "\n",
    "# 定义写入IP寄存器的函数，可以对IP的对应位置进行写入\n",
    "def write_ip_register(ip, offset, value):\n",
    "    \"\"\"\n",
    "    向指定IP核的寄存器写入值。\n",
    "    \n",
    "    参数:\n",
    "    ip -- IP核实例\n",
    "    offset -- 寄存器的偏移地址\n",
    "    value -- 要写入的值\n",
    "    \"\"\"\n",
    "    # 假设IP核实例有一个名为'write'的方法来写入寄存器\n",
    "    ip.write(offset, value)\n",
    "\n",
    "def read_ip_register(ip, offset):\n",
    "    \"\"\"\n",
    "    从指定IP核的寄存器读取值。\n",
    "    \n",
    "    参数:\n",
    "    ip -- IP核实例\n",
    "    offset -- 寄存器的偏移地址\n",
    "    \n",
    "    返回值:\n",
    "    寄存器中的值\n",
    "    \"\"\"\n",
    "    # 通过寄存器偏移地址直接访问字典属性\n",
    "    return ip.read(offset)\n",
    "\n",
    "# 定义已有的偏移类，用于存储\n",
    "class RegisterOffset:\n",
    "    # 定义每个寄存器间的间隔为8字节\n",
    "    REGISTER_OFFSET = 0x08\n",
    "    # fetch模块\n",
    "    FETCH_INSN_COUNT_OFFSET = 0x10  # fetch模块的指令数量寄存器\n",
    "    FETCH_INSN_ADDR_OFFSET = 0x18  # fetch模块的指令地址寄存器\n",
    "    # load模块\n",
    "    LOAD_INP_ADDR_OFFSET = 0x10  # load模块的输入缓冲区地址8字节64位\n",
    "    LOAD_WGT_ADDR_OFFSET = 0x1c  # load模块的权重缓冲区地址8字节64位\n",
    "    # compute模块\n",
    "    COMPUTE_DONE_OFFSET = 0x10  # compute模块的done信号\n",
    "    COMPUTE_DONE_CTRL_OFFSET = 0x14  # compute模块的done信号\n",
    "    COMPUTE_UOP_OFFSET = 0x20  # compute模块的done信号\n",
    "    COMPUTE_BIAS_OFFSET = 0x2c  # compute模块的done信号\n",
    "    # store模块\n",
    "    STORE_OUT_ADDR_OFFSET = 0x10  # store模块的输出缓冲区地址8字节64位\n",
    "\n",
    "# 从overlay获取IP实例,也就是handle\n",
    "fetch_ip = overlay.fetch_0\n",
    "load_ip = overlay.load_0\n",
    "compute_ip = overlay.compute_0\n",
    "store_ip = overlay.store_0\n",
    "\n",
    "# 查看各IP寄存器映射\n",
    "fetch_ip.register_map\n",
    "load_ip.register_map\n",
    "compute_ip.register_map\n",
    "store_ip.register_map\n",
    "\n",
    "\n",
    "# 使用写入寄存器函数，对四个IP进行配置\n",
    "# 配置和VTA不同，我们的三个缓冲区的物理起始地址是有值的，\n",
    "# 这是因为我使用memcpy时，指令中的dram_base代表的是dram的索引而不是首地址\n",
    "# 因此传入指令时要传入索引，索引按照dram存储数据大小寻址\n",
    "# 因此真正的数组首地址就是这里定义的物理地址\n",
    "def RunSaa(insn_count,\n",
    "           insn_phy_addr,\n",
    "           uop_phy_addr,\n",
    "           input_phy_addr,\n",
    "           weight_phy_addr,\n",
    "           bias_phy_addr, \n",
    "           output_phy_addr,\n",
    "           wait_cycles):\n",
    "    \"\"\"\n",
    "    向saa提交指令等待一次大批量指令执行完成,注意要有done信号表示计算完成以退出RunSaa(暂时没有)\n",
    "    \n",
    "    参数:\n",
    "    insn_count -- 这一次批量执行的指令数量\n",
    "    insn_phy_addr -- 这一次执行的指令的缓冲区首地址\n",
    "    input_phy_addr -- 这一次执行的指令的输入缓冲区首地址\n",
    "    weight_phy_addr -- 这一次执行的指令的权重缓冲区首地址\n",
    "    output_phy_addr -- 这一次执行的指令的输出缓冲区首地址\n",
    "    wait_cycles -- 最大等待的时间周期,可以设置很大很大,查询done信号等待这一批指令执行完成\n",
    "    \"\"\"\n",
    "    # 配置各IP的寄存器\n",
    "    # 配置fetch\n",
    "    write_ip_register(fetch_ip,RegisterOffset.FETCH_INSN_COUNT_OFFSET,insn_count) # 配置指令数量寄存器\n",
    "    write_ip_register(fetch_ip,RegisterOffset.FETCH_INSN_ADDR_OFFSET,insn_phy_addr) # 配置指令物理地址寄存器，也就是指令缓冲区物理首地址\n",
    "    # 配置load\n",
    "    write_ip_register(load_ip,RegisterOffset.LOAD_INP_ADDR_OFFSET,input_phy_addr) # 配置输入缓冲区物理地址\n",
    "    write_ip_register(load_ip,RegisterOffset.LOAD_WGT_ADDR_OFFSET,weight_phy_addr) # 配置权重缓冲区物理地址\n",
    "    # 配置compute\n",
    "    write_ip_register(compute_ip,RegisterOffset.COMPUTE_UOP_OFFSET,uop_phy_addr) # 配置uop缓冲区物理地址\n",
    "    write_ip_register(compute_ip,RegisterOffset.COMPUTE_BIAS_OFFSET,bias_phy_addr) # 配置bias缓冲区物理地址\n",
    "    # 配置store\n",
    "    write_ip_register(store_ip,RegisterOffset.STORE_OUT_ADDR_OFFSET,output_phy_addr) # 配置输出缓冲区物理地址\n",
    "\n",
    "    #写入各IP控制寄存器，启动IP进行计算\n",
    "    write_ip_register(fetch_ip,0x0,0x1) # 指令寄存器写入0x1启动本次模块\n",
    "    write_ip_register(load_ip,0x0,0x81) # 加载寄存器写入0x81使得模块可以多次自动启动计算指令\n",
    "    write_ip_register(compute_ip,0x0,0x81) # 计算寄存器写入0x81使得模块可以多次自动启动计算指令\n",
    "    write_ip_register(store_ip,0x0,0x81) # 存储寄存器写入0x81使得模块可以多次自动启动计算指令\n",
    "\n",
    "    #延时1微秒使得设备响应\n",
    "    time.sleep(0.000001) # 让出CPU，等待0.000001秒（1u秒）\n",
    "    \n",
    "    # 读取compute的done信号是否完成\n",
    "    for t in range(0, wait_cycles):\n",
    "        done_flag = read_ip_register(compute_ip,RegisterOffset.COMPUTE_DONE_OFFSET) # 从computeIP的done寄存器读取本次指令是否执行完毕\n",
    "        if done_flag == 0x1: # 如果done_flag被置为1，代表这次执行的是FINISH指令，本批次指令执行完毕\n",
    "            print(\"done：\",t)\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(0.0000001) # 让出CPU，等待0.000001秒（1u秒）\n",
    "\n",
    "    # 根据是否超时返回，如果没超时返回0，超时返回1\n",
    "    return 0 if t < wait_cycles else 1\n",
    "\n",
    "# # 测试写入done信号并且读取done信号\n",
    "# done_flag = read_ip_register(compute_ip,0x14) #读取done寄存器\n",
    "# print(done_flag)\n",
    "# write_ip_register(compute_ip,RegisterOffset.COMPUTE_DONE_OFFSET,0x1) # 写入1 \n",
    "# done_flag = read_ip_register(compute_ip,0x14) #读取done寄存器\n",
    "# print(done_flag)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.连续缓存申请"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义指令缓冲区大小\n",
    "insn_count = 4000 # 最多能容纳2000条指令\n",
    "block_size = 2*MATRIX_WIDTH # 以脉动阵列大小作为分块\n",
    "# 定义buffer大小,这是执行一个批量的大小\n",
    "row = 200*MATRIX_WIDTH\n",
    "col = 200*MATRIX_WIDTH\n",
    "col1 =200*MATRIX_WIDTH\n",
    "\n",
    "# 定义PS端缓冲区,不使用cache，数据类型注意\n",
    "# instruct_buffer = allocate(shape = (insn_count), cacheable = 0, dtype = Instruct_DataType)\n",
    "input_buffer = allocate(shape = (row,col), cacheable = 0, dtype = Input_DataType)\n",
    "weight_buffer = allocate(shape = (col,col1), cacheable = 0, dtype = Weight_DataType)\n",
    "output_buffer  = allocate(shape = (row,col1), cacheable = 0, dtype = Output_DataType)\n",
    "bias_buffer  = allocate(shape = (row,col1), cacheable = 0, dtype = Output_DataType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.测试数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly generated input buffer:\n",
      "[[40 92 29 ... 77 22 88]\n",
      " [40 84 87 ... 82 85 28]\n",
      " [58 12 67 ... 59 71 62]\n",
      " ...\n",
      " [69 97 27 ... 78  0 53]\n",
      " [36 41 94 ... 16 63 47]\n",
      " [52 36 67 ... 62 47 43]]\n",
      "\n",
      "Randomly generated weight buffer:\n",
      "[[15  0 34 ... 15 62 85]\n",
      " [61 64 47 ... 50 12 11]\n",
      " [32 85 93 ... 61 83 67]\n",
      " ...\n",
      " [44 53 24 ... 41 11 49]\n",
      " [85 66 12 ... 86  6 97]\n",
      " [18  4  3 ... 59 13 35]]\n",
      "\n",
      "Randomly generated bias buffer:\n",
      "[[21 25 19 ... 49 63 49]\n",
      " [51 91 25 ... 61 98 20]\n",
      " [53 56 34 ... 34 59 27]\n",
      " ...\n",
      " [56 21  7 ...  7 31 34]\n",
      " [ 0 83 40 ... 43 59 29]\n",
      " [16 29 91 ... 53 25 17]]\n"
     ]
    }
   ],
   "source": [
    "# 随机生成矩阵并存储到相应的数据缓冲区中\n",
    "np.random.seed(2)  # 设置随机种子以确保生成的随机数相同\n",
    "input_buffer[:] = np.random.randint(0, 100, size=(row, col), dtype=Input_DataType)\n",
    "weight_buffer[:] = np.random.randint(0, 100, size=(col, col1), dtype=Weight_DataType)\n",
    "bias_buffer[:] = np.random.randint(0, 100, size=(row, col1), dtype=Output_DataType)\n",
    "print(\"Randomly generated input buffer:\")\n",
    "print(input_buffer)\n",
    "print(\"\\nRandomly generated weight buffer:\")\n",
    "print(weight_buffer)\n",
    "print(\"\\nRandomly generated bias buffer:\")\n",
    "print(bias_buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载存储测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insn_idx: 2\n",
      "done： 17234\n",
      "INFO - Saa run time: 1.394785s\n",
      "INFO - 带宽: 0.469865Gbps\n",
      "Randomly generated input buffer:\n",
      "[[40 92 29 ... 77 22 88]\n",
      " [40 84 87 ... 82 85 28]\n",
      " [58 12 67 ... 59 71 62]\n",
      " ...\n",
      " [69 97 27 ... 78  0 53]\n",
      " [36 41 94 ... 16 63 47]\n",
      " [52 36 67 ... 62 47 43]]\n",
      "Randomly generated weight buffer:\n",
      "[[15  0 34 ... 15 62 85]\n",
      " [61 64 47 ... 50 12 11]\n",
      " [32 85 93 ... 61 83 67]\n",
      " ...\n",
      " [44 53 24 ... 41 11 49]\n",
      " [85 66 12 ... 86  6 97]\n",
      " [18  4  3 ... 59 13 35]]\n",
      "\n",
      "Randomly generated bias buffer:\n",
      "[[21 25 19 ... 49 63 49]\n",
      " [51 91 25 ... 61 98 20]\n",
      " [53 56 34 ... 34 59 27]\n",
      " ...\n",
      " [56 21  7 ...  7 31 34]\n",
      " [ 0 83 40 ... 43 59 29]\n",
      " [16 29 91 ... 53 25 17]]\n",
      "Randomly generated output buffer:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from pynq import allocate\n",
    "from saa_utils import *\n",
    "import time\n",
    "import numpy as np\n",
    "# 初始化指令队列\n",
    "insn_size = 3\n",
    "insn_buf = allocate(shape = (insn_size), cacheable = 0, dtype = Instruct_DataType)\n",
    "insn_idx = 0\n",
    "# 初始化微操作序列\n",
    "uop_buf = allocate(shape = (insn_size), cacheable = 0, dtype = Uop_DataType)\n",
    "\n",
    "# # 加载输入\n",
    "# insn_buf[insn_idx] = get2DLoadStoreInsn(OPCODE_LOAD, \n",
    "#                                         0,\n",
    "#                                         0,\n",
    "#                                         0,\n",
    "#                                         1,\n",
    "#                                       OUTPUT_BUFFER_ID, \n",
    "#                                       0, \n",
    "#                                       0, \n",
    "#                                       row//MATRIX_WIDTH, \n",
    "#                                       col1//MATRIX_WIDTH, \n",
    "#                                       col1//MATRIX_WIDTH)# 直接加载一整个块\n",
    "# insn_idx += 1\n",
    "\n",
    "# 存储输入\n",
    "insn_buf[insn_idx] = get2DLoadStoreInsn(OPCODE_STORE, \n",
    "                                        0,\n",
    "                                        0,\n",
    "                                        1,\n",
    "                                        0,\n",
    "                                      OUTPUT_BUFFER_ID, \n",
    "                                      0, \n",
    "                                      0, \n",
    "                                      row//MATRIX_WIDTH, \n",
    "                                      col1//MATRIX_WIDTH, \n",
    "                                      col1//MATRIX_WIDTH)# 直接加载一整个块\n",
    "insn_idx += 1\n",
    "\n",
    "# 生成结束指令\n",
    "insn_buf[insn_idx] = getFinishInsn(0,1)\n",
    "insn_idx += 1\n",
    "print(f\"insn_idx: {insn_idx}\") \n",
    "\n",
    "# 执行load和store\n",
    "# 定义一次最多等待周期为1000万周期\n",
    "wait_cycles = 100000\n",
    "# 运行SAA硬件\n",
    "pt0 = time.perf_counter()\n",
    "RunSaa(insn_idx,\n",
    "       insn_buf.physical_address,\n",
    "       uop_buf.physical_address,\n",
    "       input_buffer.physical_address,\n",
    "       weight_buffer.physical_address,\n",
    "       bias_buffer.physical_address,\n",
    "       output_buffer.physical_address,\n",
    "       wait_cycles)\n",
    "pt1 = time.perf_counter()\n",
    "t_fpga = pt1 - pt0\n",
    "print(\"INFO - Saa run time: %fs\" % t_fpga)     \n",
    "# 计算传输带宽\n",
    "data_size = row * col1 * 32 * 2\n",
    "bps = data_size/(t_fpga*1E9)\n",
    "print(f\"INFO - 带宽: {bps:.6f}Gbps\")\n",
    "\n",
    "# 检查输出\n",
    "print(\"Randomly generated input buffer:\")\n",
    "print(input_buffer)\n",
    "print(\"Randomly generated weight buffer:\")\n",
    "print(weight_buffer)\n",
    "print(\"\\nRandomly generated bias buffer:\")\n",
    "print(bias_buffer)\n",
    "print(\"Randomly generated output buffer:\")\n",
    "print(output_buffer)\n",
    "output_buffer[:]=0\n",
    "# del output_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly generated input buffer:\n",
      "[[23  1 13 ... 80 76 81]\n",
      " [37 87 78 ... 66 48 14]\n",
      " [13 51 61 ... 65 23 50]\n",
      " ...\n",
      " [70 25 61 ... 94 12 92]\n",
      " [32 75 40 ... 25 86 78]\n",
      " [16 15  5 ... 54 35 91]]\n",
      "\n",
      "Randomly generated weight buffer:\n",
      "[[56  4 28 ... 41 60 59]\n",
      " [98 88 62 ... 31 30 66]\n",
      " [ 0 12 40 ... 90 49  6]\n",
      " ...\n",
      " [24 81 37 ... 86 49  2]\n",
      " [52 77 31 ... 24 70 87]\n",
      " [94 46 55 ... 26 79 79]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (256,) into shape (160,160)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(weight_matrix)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 执行打包操作\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mpack_matrix_to_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMATRIX_WIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m pack_matrix_to_buffer(weight_matrix, MATRIX_WIDTH, weight_buffer)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPacked input buffer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/root/jupyter_notebooks/SAA_package/saa_utils.py:12\u001b[0m, in \u001b[0;36mpack_matrix_to_buffer\u001b[0;34m(matrix, block_size, buffer)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block_col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(block_col_count):\n\u001b[1;32m     11\u001b[0m     block \u001b[38;5;241m=\u001b[39m matrix[block_row\u001b[38;5;241m*\u001b[39mblock_size:(block_row\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mblock_size, block_col\u001b[38;5;241m*\u001b[39mblock_size:(block_col\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mblock_size]\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mbuffer\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbuffer_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbuffer_index\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     13\u001b[0m     buffer_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39msize\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (256,) into shape (160,160)"
     ]
    }
   ],
   "source": [
    "# 随机生成矩阵并存储到相应的数据缓冲区中\n",
    "np.random.seed(3)  # 设置随机种子以确保生成的随机数相同\n",
    "input_matrix = np.random.randint(0, 100, size=(row, col), dtype=Input_DataType)\n",
    "weight_matrix = np.random.randint(0, 100, size=(col, col1), dtype=Weight_DataType)\n",
    "print(\"Randomly generated input buffer:\")\n",
    "print(input_matrix)\n",
    "print(\"\\nRandomly generated weight buffer:\")\n",
    "print(weight_matrix)\n",
    "\n",
    "# 执行打包操作\n",
    "pack_matrix_to_buffer(input_matrix, MATRIX_WIDTH, input_buffer)\n",
    "pack_matrix_to_buffer(weight_matrix, MATRIX_WIDTH, weight_buffer)\n",
    "print(\"Packed input buffer:\")\n",
    "print(input_buffer)\n",
    "print(\"Packed input buffer:\")\n",
    "print(weight_buffer)\n",
    "\n",
    "# 将输入矩阵转换为np.int32类型，以避免溢出\n",
    "input_matrix_int32 = input_matrix.astype(np.int32)\n",
    "weight_matrix_int32 = weight_matrix.astype(np.int32)\n",
    "# 定义input_buffer和weight_buffer的矩阵乘法结果的结果矩阵\n",
    "pt0 = time.perf_counter()\n",
    "result_matrix = np.dot(input_matrix_int32, weight_matrix_int32)\n",
    "pt1 = time.perf_counter()\n",
    "time_sw = pt1 - pt0\n",
    "print(\"pure software: %fs\" % time_sw)\n",
    "# 打印矩阵乘法结果\n",
    "print(\"Matrix multiplication result:\")\n",
    "print(result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "INFO - Blocked GEMM test: dim_I=16, dim_J=16, dim_K=16, block=8, bias_use=0\n",
      "insn_idx: 6\n",
      "done: 0\n",
      "INFO - Saa run time: 0.000355s\n",
      "INFO - Synchronization time: 0.355283ms\n",
      "INFO - Throughput: 0.034587GOPs/s\n",
      "[[24987 29480 25807 28555 34062 35328 40731 29885 35518 42848 45164 34488\n",
      "  38598 39159 42572 33418]\n",
      " [27965 32766 27694 13375 20304 39995 27828 24688 28638 45081 38295 22877\n",
      "  33310 42301 37854 29117]\n",
      " [23178 19190 22626 30452 25476 26291 25811 38416 35412 31074 27533 38223\n",
      "  32001 31627 36325 44065]\n",
      " [23071 30627 28060 22068 26828 35212 33943 36255 30317 48155 39227 34637\n",
      "  28905 46872 37960 40761]\n",
      " [30109 33346 28580 29936 39235 36115 40943 32164 28696 28542 35681 30552\n",
      "  38936 39875 47241 38118]\n",
      " [33739 37535 32295 21067 29017 44383 35203 23209 24798 31369 32104 18886\n",
      "  24559 45479 35422 18750]\n",
      " [25283 24686 25860 33029 31738 24161 32013 44321 23136 22893 28179 28754\n",
      "  26734 29830 38431 43551]\n",
      " [24698 37175 32950 29071 31642 35284 34913 35455 20099 35092 22651 26708\n",
      "  35876 45817 37814 35370]\n",
      " [36179 38809 39842 35367 47385 49312 46354 42449 42438 49702 48309 42097\n",
      "  42033 41578 45538 41775]\n",
      " [32706 41490 36688 21510 44371 54911 48756 31569 36417 52741 42628 30503\n",
      "  32666 42954 42178 17257]\n",
      " [27618 30314 31611 39858 37853 40762 45985 50622 36957 38658 42275 50469\n",
      "  28101 35663 38055 43721]\n",
      " [29854 44577 32527 33818 39077 56845 49147 43080 36738 58060 49462 41770\n",
      "  36256 50396 35630 32844]\n",
      " [39898 35779 43212 28823 41068 41256 44884 33904 28692 31952 28437 28210\n",
      "  34929 37646 40390 29465]\n",
      " [30472 39989 36905 19687 34611 45725 41772 26352 22730 31329 29470 15897\n",
      "  28305 38145 37877 21862]\n",
      " [27147 33927 37932 45729 35092 36004 38082 42994 22696 24466 21858 26720\n",
      "  32760 31982 30503 37735]\n",
      " [35908 43053 38845 36232 35136 50259 41062 35399 23120 38619 22976 22358\n",
      "  30182 46339 36359 32252]]\n"
     ]
    }
   ],
   "source": [
    "# 执行分块矩阵乘法\n",
    "blocked_gemm_test(saa_driver,\n",
    "              row, \n",
    "              col1, \n",
    "              col, \n",
    "              input_buffer, \n",
    "              weight_buffer,\n",
    "              bias_buffer, \n",
    "              output_buffer, \n",
    "              block_size, \n",
    "              0)\n",
    "# 检查输出\n",
    "print(output_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 回收缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看完成后清空缓冲区\n",
    "del output_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct_buffer = allocate(shape = (1), cacheable = 0, dtype = Instruct_DataType)\n",
    "# instructions = [] #临时存储指令\n",
    "# # insn_test = getWeightPreloadComputeInsn(\n",
    "# #     1,\n",
    "# #     1,\n",
    "# #     1,\n",
    "# #     1,\n",
    "# #     1,\n",
    "# #     1)\n",
    "\n",
    "# # insn_test = getWeightPreloadInsn(1, 1)\n",
    "\n",
    "# # insn_test = get2DLoadStoreInsn(\n",
    "# #                           1, \n",
    "# #                           1, \n",
    "# #                           1, \n",
    "# #                           1, \n",
    "# #                           1, \n",
    "# #                           1, \n",
    "# #                           1)\n",
    "\n",
    "# insn_test = getComputeInsn(1, \n",
    "#                         1, \n",
    "#                         1, \n",
    "#                         1)\n",
    "    \n",
    "# instructions.append(insn_test)\n",
    "# # 将生成的指令批量存入缓冲区，然后启动saa让其读取指令\n",
    "# for i, instruction in enumerate(instructions):\n",
    "#     instruct_buffer[i] = np.frombuffer(instruction, dtype=Instruct_DataType)\n",
    "#     print(instruct_buffer[i])\n",
    "#     print_binary(instruct_buffer[i]) # 输出指令的二进制表示\n",
    "\n",
    "\n",
    "# from pynq import allocate\n",
    "# import time\n",
    "# import numpy as np\n",
    "# wait_cycles = 100000 # 定义一次最多等待周期为1000万周期\n",
    "# def blocked_gemm_test(saa_driver,\n",
    "#               dim_I, \n",
    "#               dim_J, \n",
    "#               dim_K, \n",
    "#               input, \n",
    "#               weight,\n",
    "#               bias, \n",
    "#               output, \n",
    "#               block, \n",
    "#               bias_use):\n",
    "    \n",
    "#     print(\"=====================================================================================\")\n",
    "#     print(f\"INFO - Blocked GEMM test: dim_I={dim_I}, dim_J={dim_J}, dim_K={dim_K}, block={block}, bias_use={bias_use}\")\n",
    "    \n",
    "#     # 计算分块\n",
    "#     dim_I_block = dim_I // MATRIX_WIDTH\n",
    "#     dim_J_block = dim_J // MATRIX_WIDTH\n",
    "#     dim_K_block = dim_K // MATRIX_WIDTH\n",
    "\n",
    "#     # 计算指令数量\n",
    "#     insn_load_size = (dim_I_block * dim_K_block) + (dim_J_block * dim_K_block)\n",
    "#     insn_compute_size = 2 * dim_I_block * dim_K_block * dim_J_block  # 不使用权重复用\n",
    "# #     insn_compute_size = (dim_I_block + 1) * dim_K_block * dim_J_block  # 使用权重复用\n",
    "# #     insn_compute_size = dim_I_block * dim_K_block * dim_J_block + 1  # 使用权重复用和双缓冲\n",
    "#     insn_store_size = dim_I_block * dim_J_block\n",
    "#     insn_size = insn_load_size + insn_store_size + insn_compute_size + 1\n",
    "\n",
    "#     # 初始化指令队列\n",
    "#     insn_buf = allocate(shape = (insn_size), cacheable = 0, dtype = Instruct_DataType)\n",
    "#     insn_idx = 0\n",
    "    \n",
    "#     # 生成加载Input指令\n",
    "#     for i in range(dim_I_block):\n",
    "#         for k in range(dim_K_block):\n",
    "#             buffer_start = 0\n",
    "#             dram_start = 0\n",
    "#             A_block = i*dim_K_block+k\n",
    "#             buffer_offset = buffer_start + A_block * MATRIX_WIDTH\n",
    "#             dram_offset = dram_start + i * dim_K_block * MATRIX_WIDTH * MATRIX_WIDTH + k * MATRIX_WIDTH\n",
    "#             insn_buf[insn_idx] = get2DLoadStoreInsn(OPCODE_LOAD, \n",
    "#                                       INPUT_BUFFER_ID, \n",
    "#                                       buffer_offset, \n",
    "#                                       dram_offset, \n",
    "#                                       MATRIX_WIDTH, \n",
    "#                                       MATRIX_WIDTH, \n",
    "#                                       dim_K)\n",
    "#             insn_idx += 1\n",
    "\n",
    "#     # 生成加载weight指令\n",
    "#     for k in range(dim_K_block):\n",
    "#         for j in range(dim_J_block):\n",
    "#             buffer_start = 0\n",
    "#             dram_start = 0\n",
    "#             A_block = k * dim_J_block + j\n",
    "#             buffer_offset = buffer_start + A_block * MATRIX_WIDTH\n",
    "#             dram_offset = dram_start + k * dim_J_block * MATRIX_WIDTH * MATRIX_WIDTH + j * MATRIX_WIDTH \n",
    "#             insn_buf[insn_idx] = get2DLoadStoreInsn(OPCODE_LOAD, \n",
    "#                                       WEIGHT_BUFFER_ID, \n",
    "#                                       buffer_offset, \n",
    "#                                       dram_offset, \n",
    "#                                       MATRIX_WIDTH, \n",
    "#                                       MATRIX_WIDTH, \n",
    "#                                       dim_J)\n",
    "#             insn_idx += 1\n",
    "    \n",
    "#     # 生成计算指令\n",
    "#     # 用于切换权重寄存器，最先使用 weight1\n",
    "#     pingpang = 0\n",
    "#     wb_start_addr = 0\n",
    "#     input_start_addr = 0\n",
    "#     output_start_addr = 0\n",
    "#     weight_offset = 0\n",
    "#     output_offset = 0\n",
    "#     input_offset = 0\n",
    "#     accumulate = 0\n",
    "    \n",
    "#     # 初始化指令计数\n",
    "#     compute_count = insn_idx\n",
    "\n",
    "#     # 迭代公共维度块和输出列块\n",
    "#     for k in range(dim_K_block):\n",
    "#         for j in range(dim_J_block):\n",
    "#             # 计算权重偏移\n",
    "#             weight_offset = wb_start_addr + (k * dim_J_block + j) * MATRIX_WIDTH\n",
    "#             accumulate = 0 if k == 0 else 1\n",
    "\n",
    "#             # 第一次加载权重，使用初始寄存器，无法双缓冲\n",
    "#             if k == 0 and j == 0:\n",
    "#                 insn_buf[insn_idx] = getWeightPreloadInsn(weight_offset, pingpang)\n",
    "#                 insn_idx += 1\n",
    "#             else:\n",
    "#                 # 剩下的权重加载可以进行双缓冲\n",
    "# #                 insn_buf[insn_idx] = getWeightPreloadComputeInsn(\n",
    "# #                     input_offset,\n",
    "# #                     weight_offset,\n",
    "# #                     output_offset,\n",
    "# #                     pingpang,\n",
    "# #                     pingpang,\n",
    "# #                     accumulate)\n",
    "# #                 insn_idx += 1\n",
    "    \n",
    "# #                 insn_buf[insn_idx] = getComputeInsn(input_offset, \n",
    "# #                                         output_offset, \n",
    "# #                                         pingpang, \n",
    "# #                                         accumulate)\n",
    "# #                 insn_idx += 1\n",
    "#                 insn_buf[insn_idx] = getComputeInsn(input_offset, \n",
    "#                                         output_offset, \n",
    "#                                         pingpang, \n",
    "#                                         accumulate)\n",
    "#                 insn_idx += 1\n",
    "#                 insn_buf[insn_idx] = getWeightPreloadInsn(weight_offset, not pingpang)\n",
    "#                 insn_idx += 1\n",
    "                \n",
    "#                 pingpang = not pingpang  # 切换加载寄存器和计算寄存器\n",
    "\n",
    "\n",
    "#             # 迭代输出行块\n",
    "#             for i in range(dim_I_block):\n",
    "#                 output_offset = output_start_addr + (i * dim_J_block + j) * MATRIX_WIDTH\n",
    "#                 input_offset = input_start_addr + (i * dim_K_block + k) * MATRIX_WIDTH\n",
    "\n",
    "#                 # 如果不是最后一个计算，使用 getComputeInsn 计算\n",
    "#                 if i != dim_I_block - 1:\n",
    "#                     insn_buf[insn_idx] = getComputeInsn(input_offset, \n",
    "#                                             output_offset, \n",
    "#                                             pingpang, \n",
    "#                                             accumulate)\n",
    "#                     insn_idx += 1\n",
    "#                 # 如果是最后一个权重块，使用当前寄存器进行计算\n",
    "#                 if i == dim_I_block - 1 and j == dim_J_block - 1 and k == dim_K_block - 1:\n",
    "#                     insn_buf[insn_idx] = getComputeInsn(input_offset, \n",
    "#                                             output_offset, \n",
    "#                                             pingpang, \n",
    "#                                             accumulate)\n",
    "#                     insn_idx += 1\n",
    "# #             if k != 0 and j != 0:\n",
    "# #             insn_buf[insn_idx] = getComputeInsn(input_offset, \n",
    "# #                                     output_offset, \n",
    "# #                                     pingpang, \n",
    "# #                                     accumulate)\n",
    "#             insn_idx += 1\n",
    "    \n",
    "    \n",
    "#     # 更新计算指令的数量\n",
    "#     compute_count = insn_idx - compute_count\n",
    "#     print(f\"compute_count: {compute_count}\")    \n",
    "\n",
    "    \n",
    "#     # 生成存储指令\n",
    "#     for i in range(dim_I_block):\n",
    "#         for j in range(dim_J_block):\n",
    "#             buffer_start = 0\n",
    "#             dram_start = 0\n",
    "#             A_block = i * dim_J_block + j\n",
    "#             buffer_offset = buffer_start + A_block * MATRIX_WIDTH\n",
    "#             dram_offset = dram_start + i * dim_J_block * MATRIX_WIDTH * MATRIX_WIDTH + j * MATRIX_WIDTH\n",
    "#             insn_buf[insn_idx] = get2DLoadStoreInsn(OPCODE_STORE, \n",
    "#                                        OUTPUT_BUFFER_ID, \n",
    "#                                        buffer_offset, \n",
    "#                                        dram_offset, \n",
    "#                                        MATRIX_WIDTH, \n",
    "#                                        MATRIX_WIDTH, \n",
    "#                                        dim_J)\n",
    "#             insn_idx += 1\n",
    "            \n",
    "#     # 生成结束指令\n",
    "#     insn_buf[insn_idx] = getFinishInsn()\n",
    "#     insn_idx += 1\n",
    "\n",
    "#     print(\"insn_size\",insn_size)\n",
    "#     print(\"insn_idx\",insn_idx)\n",
    "#     print(\"insn\",insn_idx)\n",
    "#     for i in range(insn_idx):\n",
    "#         if i>=insn_load_size and i<insn_load_size+compute_count:\n",
    "#             print_binary(insn_buf[i])\n",
    "            \n",
    "#     # 运行SAA硬件\n",
    "#     pt0 = time.perf_counter()\n",
    "#     saa_driver.run_saa(insn_idx,\n",
    "#            insn_buf.physical_address,\n",
    "#            input.physical_address,\n",
    "#            weight.physical_address,\n",
    "#            output.physical_address,\n",
    "#            wait_cycles)\n",
    "#     pt1 = time.perf_counter()\n",
    "#     time_sw = pt1 - pt0\n",
    "#     print(\"saa run time: %fs\" % time_sw)     \n",
    "    \n",
    "#     # 计算吞吐量\n",
    "    \n",
    "    \n",
    "#     return 0 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
