{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAA_top测试\n",
    "## 1. 加载Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "[[  0   1   2   3   4   5   6   7   8   9  10]\n",
      " [ 11  12  13  14  15  16  17  18  19  20  21]\n",
      " [ 22  23  24  25  26  27  28  29  30  31  32]\n",
      " [ 33  34  35  36  37  38  39  40  41  42  43]\n",
      " [ 44  45  46  47  48  49  50  51  52  53  54]\n",
      " [ 55  56  57  58  59  60  61  62  63  64  65]\n",
      " [ 66  67  68  69  70  71  72  73  74  75  76]\n",
      " [ 77  78  79  80  81  82  83  84  85  86  87]\n",
      " [ 88  89  90  91  92  93  94  95  96  97  98]\n",
      " [ 99 100 101 102 103 104 105 106 107 108 109]\n",
      " [110 111 112 113 114 115 116 117 118 119 120]]\n",
      "Padded matrix:\n",
      "[[  0   1   2   3   4   5   6   7   8   9  10   0]\n",
      " [ 11  12  13  14  15  16  17  18  19  20  21   0]\n",
      " [ 22  23  24  25  26  27  28  29  30  31  32   0]\n",
      " [ 33  34  35  36  37  38  39  40  41  42  43   0]\n",
      " [ 44  45  46  47  48  49  50  51  52  53  54   0]\n",
      " [ 55  56  57  58  59  60  61  62  63  64  65   0]\n",
      " [ 66  67  68  69  70  71  72  73  74  75  76   0]\n",
      " [ 77  78  79  80  81  82  83  84  85  86  87   0]\n",
      " [ 88  89  90  91  92  93  94  95  96  97  98   0]\n",
      " [ 99 100 101 102 103 104 105 106 107 108 109   0]\n",
      " [110 111 112 113 114 115 116 117 118 119 120   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "Buffer content (flattened):\n",
      "[  0   1   2   3  11  12  13  14  22  23  24  25  33  34  35  36   4   5\n",
      "   6   7  15  16  17  18  26  27  28  29  37  38  39  40   8   9  10   0\n",
      "  19  20  21   0  30  31  32   0  41  42  43   0  44  45  46  47  55  56\n",
      "  57  58  66  67  68  69  77  78  79  80  48  49  50  51  59  60  61  62\n",
      "  70  71  72  73  81  82  83  84  52  53  54   0  63  64  65   0  74  75\n",
      "  76   0  85  86  87   0  88  89  90  91  99 100 101 102 110 111 112 113\n",
      "   0   0   0   0  92  93  94  95 103 104 105 106 114 115 116 117   0   0\n",
      "   0   0  96  97  98   0 107 108 109   0 118 119 120   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pynq import allocate\n",
    "\n",
    "# 假设矩阵和块的大小\n",
    "matrix_rows, matrix_cols = 11, 11  # 矩阵大小，这次尝试一个不能整除的情况\n",
    "block_size = 4  # 块大小为 2x2\n",
    "\n",
    "# 创建一个示例矩阵\n",
    "matrix = np.arange(matrix_rows * matrix_cols).reshape(matrix_rows, matrix_cols)\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "# 计算填充后的新大小\n",
    "new_rows = matrix_rows + (block_size - matrix_rows % block_size) % block_size\n",
    "new_cols = matrix_cols + (block_size - matrix_cols % block_size) % block_size\n",
    "\n",
    "# 创建填充后的矩阵\n",
    "padded_matrix = np.zeros((new_rows, new_cols), dtype=matrix.dtype)\n",
    "padded_matrix[:matrix_rows, :matrix_cols] = matrix  # 将原矩阵复制到填充矩阵中\n",
    "print(\"Padded matrix:\")\n",
    "print(padded_matrix)\n",
    "\n",
    "# 分配连续缓冲区，这次根据填充后的尺寸来分配\n",
    "buffer_size = new_rows * new_cols  # 缓冲区大小等于填充后矩阵的元素个数\n",
    "buffer = allocate(shape=(buffer_size,), dtype=np.int32)\n",
    "\n",
    "# 矩阵按块大小分割并打包到缓冲区\n",
    "def pack_matrix_to_buffer(matrix, block_size, buffer):\n",
    "    rows, cols = matrix.shape\n",
    "    buffer_index = 0\n",
    "    for block_row in range(0, rows, block_size):\n",
    "        for block_col in range(0, cols, block_size):\n",
    "            # 提取块数据，使用np.pad确保块的大小为block_size x block_size\n",
    "            block = matrix[block_row:block_row+block_size, block_col:block_col+block_size]\n",
    "            if block.shape[0] < block_size or block.shape[1] < block_size:\n",
    "                # 如果块小于block_size，使用0填充。\n",
    "                block = np.pad(block, ((0, block_size-block.shape[0]), (0, block_size-block.shape[1])), 'constant')\n",
    "            buffer[buffer_index:buffer_index+block.size] = block.flatten() #二维块降到一维填充\n",
    "            buffer_index += block.size\n",
    "\n",
    "# 执行打包操作\n",
    "pack_matrix_to_buffer(padded_matrix, block_size, buffer)\n",
    "\n",
    "# 将连续缓冲区的内容打印出来，以验证结果\n",
    "print(\"Buffer content (flattened):\")\n",
    "print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saa_top Overlay downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pynq import allocate\n",
    "import random\n",
    "import time\n",
    "import saa_top_driver\n",
    "from saa_insn_driver import * \n",
    "from saa_utils import * \n",
    "# 创建 SaaDriver 实例\n",
    "saa_driver = saa_top_driver.SaaDriver(\"saa_top.bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.连续缓存申请"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义指令缓冲区大小\n",
    "insn_count = 4000 # 最多能容纳2000条指令\n",
    "block_size = 2*MATRIX_WIDTH # 以脉动阵列大小作为分块\n",
    "# 定义buffer大小,这是执行一个批量的大小\n",
    "row =  6*MATRIX_WIDTH\n",
    "col =  6*MATRIX_WIDTH\n",
    "col1 = 6*MATRIX_WIDTH\n",
    "\n",
    "# 定义PS端缓冲区,不使用cache，数据类型注意\n",
    "# instruct_buffer = allocate(shape = (insn_count), cacheable = 0, dtype = Instruct_DataType)\n",
    "input_buffer = allocate(shape = (row*col), cacheable = 0, dtype = Input_DataType)\n",
    "weight_buffer = allocate(shape = (col*col1), cacheable = 0, dtype = Weight_DataType)\n",
    "bias_buffer  = allocate(shape = (row*col1), cacheable = 0, dtype = Output_DataType)\n",
    "output_buffer  = allocate(shape = (row*col1), cacheable = 0, dtype = Output_DataType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.测试数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly generated input buffer:\n",
      "[[40 92 29 15 10 97 47 25]\n",
      " [35  6 72 22 46 54 12 43]\n",
      " [30 82 73 75  4 24 57 29]\n",
      " [ 7 45 14 82 34 82 16 84]\n",
      " [49  1  8 39 95 90 99 52]\n",
      " [75 42 50 85 10  8 30 47]\n",
      " [20 53 30 63 43 54 76 31]\n",
      " [52 90 74 78 68 20 25 33]]\n",
      "\n",
      "Randomly generated weight buffer:\n",
      "[[32  2 31 78 81 37 91  7]\n",
      " [39 74 90 46 67 84 34 52]\n",
      " [33  4 91  3 42 83 56 22]\n",
      " [55 47 51 80 65 56 57 11]\n",
      " [73 38 44 66 31 30 90 33]\n",
      " [62 84 11 58 78  6 83 69]\n",
      " [64 56 88 67 93 78 45 69]\n",
      " [37 99 20 88 84 57 68 78]]\n",
      "\n",
      "Randomly generated bias buffer:\n",
      "[[46 70 95 83 31 66 80 52]\n",
      " [76 50  4 90 63 79 49 39]\n",
      " [46  8 50 15  8 17 22 73]\n",
      " [57 90 62 83 96 43 32 26]\n",
      " [ 8 76 10 40 34 60  9 70]\n",
      " [86 70 19 56 82  1 68 40]\n",
      " [81 61 70 97 18 84 90 87]\n",
      " [22 43 52 74 72 90 99 91]]\n",
      "Packed input buffer:\n",
      "[40 92 29 15 35  6 72 22 30 82 73 75  7 45 14 82 10 97 47 25 46 54 12 43\n",
      "  4 24 57 29 34 82 16 84 49  1  8 39 75 42 50 85 20 53 30 63 52 90 74 78\n",
      " 95 90 99 52 10  8 30 47 43 54 76 31 68 20 25 33]\n",
      "Packed weight buffer:\n",
      "[32  2 31 78 39 74 90 46 33  4 91  3 55 47 51 80 81 37 91  7 67 84 34 52\n",
      " 42 83 56 22 65 56 57 11 73 38 44 66 62 84 11 58 64 56 88 67 37 99 20 88\n",
      " 31 30 90 33 78  6 83 69 93 78 45 69 84 57 68 78]\n",
      "Packed bias buffer:\n",
      "[46 70 95 83 76 50  4 90 46  8 50 15 57 90 62 83 31 66 80 52 63 79 49 39\n",
      "  8 17 22 73 96 43 32 26  8 76 10 40 86 70 19 56 81 61 70 97 22 43 52 74\n",
      " 34 60  9 70 82  1 68 40 18 84 90 87 72 90 99 91]\n",
      "pure software: 0.000414s\n",
      "Matrix multiplication result:\n",
      "[[17373 21414 19162 20357 25975 18494 22093 18135]\n",
      " [14081 13099 13837 15828 18120 14177 20810 11848]\n",
      " [17239 18184 24864 20373 25606 24637 20792 14961]\n",
      " [18706 24736 15271 24765 25590 17384 23955 18061]\n",
      " [24799 23975 19258 29751 30481 18881 31184 21302]\n",
      " [15334 14908 19117 22058 24268 20581 22076 11402]\n",
      " [19741 20599 21197 23015 25761 20603 22863 17143]\n",
      " [20953 19700 26548 24959 27257 26075 27630 15544]]\n"
     ]
    }
   ],
   "source": [
    "# 随机生成矩阵并存储到相应的数据缓冲区中\n",
    "np.random.seed(2)  # 设置随机种子以确保生成的随机数相同\n",
    "input_matrix = np.random.randint(0, 100, size=(row, col), dtype=Input_DataType)\n",
    "weight_matrix = np.random.randint(0, 100, size=(col, col1), dtype=Weight_DataType)\n",
    "bias_matrix = np.random.randint(0, 100, size=(row, col1), dtype=Output_DataType)\n",
    "print(\"Randomly generated input buffer:\")\n",
    "print(input_matrix)\n",
    "print(\"\\nRandomly generated weight buffer:\")\n",
    "print(weight_matrix)\n",
    "print(\"\\nRandomly generated bias buffer:\")\n",
    "print(bias_matrix)\n",
    "\n",
    "# 执行打包操作\n",
    "pack_matrix_to_buffer(input_matrix, MATRIX_WIDTH, input_buffer)\n",
    "pack_matrix_to_buffer(weight_matrix, MATRIX_WIDTH, weight_buffer)\n",
    "pack_matrix_to_buffer(bias_matrix, MATRIX_WIDTH, bias_buffer)\n",
    "print(\"Packed input buffer:\")\n",
    "print(input_buffer)\n",
    "print(\"Packed weight buffer:\")\n",
    "print(weight_buffer)\n",
    "print(\"Packed bias buffer:\")\n",
    "print(bias_buffer)\n",
    "\n",
    "# 将输入矩阵转换为np.int32类型，以避免溢出\n",
    "input_matrix_int32 = input_matrix.astype(np.int32)\n",
    "weight_matrix_int32 = weight_matrix.astype(np.int32)\n",
    "bias_matrix_int32 = bias_matrix.astype(np.int32)\n",
    "# 定义input_buffer和weight_buffer的矩阵乘法结果的结果矩阵\n",
    "pt0 = time.perf_counter()\n",
    "result_matrix = np.dot(input_matrix_int32, weight_matrix_int32) + bias_matrix_int32\n",
    "pt1 = time.perf_counter()\n",
    "time_sw = pt1 - pt0\n",
    "print(\"pure software: %fs\" % time_sw)\n",
    "# 打印矩阵乘法结果\n",
    "print(\"Matrix multiplication result:\")\n",
    "print(result_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分块矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "INFO - Blocked GEMM test: dim_I=8, dim_J=8, dim_K=8, block=8, bias_use=1\n",
      "insn_idx: 7\n",
      "done: 3\n",
      "INFO - Saa run time: 0.001419s\n",
      "INFO - Synchronization time: 1.418724ms\n",
      "INFO - Throughput: 0.001083GOPs/s\n",
      "output_buffer result:\n",
      "[17373 21414 19162 20357 14081 13099 13837 15828 17239 18184 24864 20373\n",
      " 18706 24736 15271 24765 25975 18494 22093 18135 18120 14177 20810 11848\n",
      " 25606 24637 20792 14961 25590 17384 23955 18061 24799 23975 19258 29751\n",
      " 15334 14908 19117 22058 19741 20599 21197 23015 20953 19700 26548 24959\n",
      " 30481 18881 31184 21302 24268 20581 22076 11402 25761 20603 22863 17143\n",
      " 27257 26075 27630 15544]\n",
      "un_pack result:\n",
      "[[17373 21414 19162 20357 25975 18494 22093 18135]\n",
      " [14081 13099 13837 15828 18120 14177 20810 11848]\n",
      " [17239 18184 24864 20373 25606 24637 20792 14961]\n",
      " [18706 24736 15271 24765 25590 17384 23955 18061]\n",
      " [24799 23975 19258 29751 30481 18881 31184 21302]\n",
      " [15334 14908 19117 22058 24268 20581 22076 11402]\n",
      " [19741 20599 21197 23015 25761 20603 22863 17143]\n",
      " [20953 19700 26548 24959 27257 26075 27630 15544]]\n"
     ]
    }
   ],
   "source": [
    "# 执行分块矩阵乘法\n",
    "blocked_gemm_test(saa_driver,\n",
    "              row, \n",
    "              col1, \n",
    "              col, \n",
    "              input_buffer, \n",
    "              weight_buffer,\n",
    "              bias_buffer, \n",
    "              output_buffer, \n",
    "              block_size, \n",
    "              1)\n",
    "# 解包并检查输出\n",
    "print(\"output_buffer result:\")\n",
    "print(output_buffer)\n",
    "output_matrix = np.zeros((row, col1), dtype=Output_DataType)\n",
    "unpack_buffer_to_matrix(output_matrix, MATRIX_WIDTH, output_buffer)\n",
    "print(\"un_pack result:\")\n",
    "print(output_matrix)\n",
    "output_buffer[:]=0\n",
    "# del output_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly generated input buffer:\n",
      "[[23  1 13 24  6 27 18 44 73 53  3  6 87 56 59 74]\n",
      " [72 99 15 31  0 25 69  2 21 99 62 17 43 24 19 74]\n",
      " [99 74 43 76  4 91 38 32 41 70 10 10 94 13 53 21]\n",
      " [27 63 44 13 38 23 96 88 20 95 49 44 37  7 93 54]\n",
      " [37 39  5 14 25  3 77 46 38 26 14 48 55 81 74 71]\n",
      " [90  5 25 90 22 10 31 45 66 99 29  2 73 66 23 63]\n",
      " [31 78 60 45 35  6  1 50 13 68 51 15 17 15 90  3]\n",
      " [65 69 94 94 66 27  1 18 68 97  2 13 29 25 21 72]\n",
      " [38 78 24 49 57 44 22 62 19 65  7 42 97 70 40 40]\n",
      " [43 44 97 33 30 11 89 65 79 53 66 71 48 80 76 81]\n",
      " [37 87 78 20 28 65 94 59 82 56 49 31 21 21 82 99]\n",
      " [ 1 55 78 86 54 70  5 28 86 90 31  9 36 54 86 33]\n",
      " [44  0 68 71 83 64 34 32 21 79 18 33  3 88 69 36]\n",
      " [63 55 68 96 37 62 57 56  8 27 23 56 71 69 39 37]\n",
      " [ 1 73 38 78 14 46 62 44 54 33  2 15 92  8  5  1]\n",
      " [44 29 57 85 31 86 80 48 17 51 17 16 74 12 62  7]]\n",
      "\n",
      "Randomly generated weight buffer:\n",
      "[[31 68 75 16 28 99 50 74 91 10 15 52 16 42 89 84]\n",
      " [39 80 75 92 32 78 37 41 16 43 20 22 19 94 18 41]\n",
      " [18 72 75 31 44 79 55  8 41 71 96 35 70 95 95 28]\n",
      " [98 13 80 64 19 37 69 21 52 30 69 53 66 48 14 13]\n",
      " [51 61 55 83 16 33 77 23  3 52 78 87 41 84 22 80]\n",
      " [49 66 76  2  8 38 28 20 55 92 36 88 66 92 90 42]\n",
      " [53 70 30  4 44 66 42 63 48 56 19 59 48 74 83 78]\n",
      " [54 32 48  5 80 65 36 95 91  2 88 97 10 43 21 49]\n",
      " [66 82  0 87 56 71 69 10 30 34 18 39 33 83 29 25]\n",
      " [83 32 92  7  5 48  6  9  1 36 33 92 64 15 37 97]\n",
      " [77 31 62 60  9 25 78 88 58 55 74 81 11 17  7 59]\n",
      " [84 12  5 10 43  4 49 17  1 80 38  5 38 76 40 41]\n",
      " [ 6 68 19 84 57 44 77  1 44 31  3  4 19 62 24 16]\n",
      " [59 52 36 17 98 82 39 19 18 50 36 70 65 23 50 35]\n",
      " [18 45 45 92 96 38 96  4 52 32 25  1 13 81 49 18]\n",
      " [25 33 63 63 12 64  2 28 35 13 87 94 64 29 99 33]]\n",
      "Packed input buffer:\n",
      "[23  1 13 24 72 99 15 31 99 74 43 76 27 63 44 13  6 27 18 44  0 25 69  2\n",
      "  4 91 38 32 38 23 96 88 73 53  3  6 21 99 62 17 41 70 10 10 20 95 49 44\n",
      " 87 56 59 74 43 24 19 74 94 13 53 21 37  7 93 54 37 39  5 14 90  5 25 90\n",
      " 31 78 60 45 65 69 94 94 25  3 77 46 22 10 31 45 35  6  1 50 66 27  1 18\n",
      " 38 26 14 48 66 99 29  2 13 68 51 15 68 97  2 13 55 81 74 71 73 66 23 63\n",
      " 17 15 90  3 29 25 21 72 38 78 24 49 43 44 97 33 37 87 78 20  1 55 78 86\n",
      " 57 44 22 62 30 11 89 65 28 65 94 59 54 70  5 28 19 65  7 42 79 53 66 71\n",
      " 82 56 49 31 86 90 31  9 97 70 40 40 48 80 76 81 21 21 82 99 36 54 86 33\n",
      " 44  0 68 71 63 55 68 96  1 73 38 78 44 29 57 85 83 64 34 32 37 62 57 56\n",
      " 14 46 62 44 31 86 80 48 21 79 18 33  8 27 23 56 54 33  2 15 17 51 17 16\n",
      "  3 88 69 36 71 69 39 37 92  8  5  1 74 12 62  7]\n",
      "Packed input buffer:\n",
      "[31 68 75 16 39 80 75 92 18 72 75 31 98 13 80 64 28 99 50 74 32 78 37 41\n",
      " 44 79 55  8 19 37 69 21 91 10 15 52 16 43 20 22 41 71 96 35 52 30 69 53\n",
      " 16 42 89 84 19 94 18 41 70 95 95 28 66 48 14 13 51 61 55 83 49 66 76  2\n",
      " 53 70 30  4 54 32 48  5 16 33 77 23  8 38 28 20 44 66 42 63 80 65 36 95\n",
      "  3 52 78 87 55 92 36 88 48 56 19 59 91  2 88 97 41 84 22 80 66 92 90 42\n",
      " 48 74 83 78 10 43 21 49 66 82  0 87 83 32 92  7 77 31 62 60 84 12  5 10\n",
      " 56 71 69 10  5 48  6  9  9 25 78 88 43  4 49 17 30 34 18 39  1 36 33 92\n",
      " 58 55 74 81  1 80 38  5 33 83 29 25 64 15 37 97 11 17  7 59 38 76 40 41\n",
      "  6 68 19 84 59 52 36 17 18 45 45 92 25 33 63 63 57 44 77  1 98 82 39 19\n",
      " 96 38 96  4 12 64  2 28 44 31  3  4 18 50 36 70 52 32 25  1 35 13 87 94\n",
      " 19 62 24 16 65 23 50 35 13 81 49 18 64 29 99 33]\n",
      "pure software: 0.000393s\n",
      "Matrix multiplication result:\n",
      "[[24987 29480 25807 28555 27965 32766 27694 13375 23178 19190 22626 30452\n",
      "  23071 30627 28060 22068]\n",
      " [34062 35328 40731 29885 20304 39995 27828 24688 25476 26291 25811 38416\n",
      "  26828 35212 33943 36255]\n",
      " [35518 42848 45164 34488 28638 45081 38295 22877 35412 31074 27533 38223\n",
      "  30317 48155 39227 34637]\n",
      " [38598 39159 42572 33418 33310 42301 37854 29117 32001 31627 36325 44065\n",
      "  28905 46872 37960 40761]\n",
      " [30109 33346 28580 29936 33739 37535 32295 21067 25283 24686 25860 33029\n",
      "  24698 37175 32950 29071]\n",
      " [39235 36115 40943 32164 29017 44383 35203 23209 31738 24161 32013 44321\n",
      "  31642 35284 34913 35455]\n",
      " [28696 28542 35681 30552 24798 31369 32104 18886 23136 22893 28179 28754\n",
      "  20099 35092 22651 26708]\n",
      " [38936 39875 47241 38118 24559 45479 35422 18750 26734 29830 38431 43551\n",
      "  35876 45817 37814 35370]\n",
      " [36179 38809 39842 35367 32706 41490 36688 21510 27618 30314 31611 39858\n",
      "  29854 44577 32527 33818]\n",
      " [47385 49312 46354 42449 44371 54911 48756 31569 37853 40762 45985 50622\n",
      "  39077 56845 49147 43080]\n",
      " [42438 49702 48309 42097 36417 52741 42628 30503 36957 38658 42275 50469\n",
      "  36738 58060 49462 41770]\n",
      " [42033 41578 45538 41775 32666 42954 42178 17257 28101 35663 38055 43721\n",
      "  36256 50396 35630 32844]\n",
      " [39898 35779 43212 28823 30472 39989 36905 19687 27147 33927 37932 45729\n",
      "  35908 43053 38845 36232]\n",
      " [41068 41256 44884 33904 34611 45725 41772 26352 35092 36004 38082 42994\n",
      "  35136 50259 41062 35399]\n",
      " [28692 31952 28437 28210 22730 31329 29470 15897 22696 24466 21858 26720\n",
      "  23120 38619 22976 22358]\n",
      " [34929 37646 40390 29465 28305 38145 37877 21862 32760 31982 30503 37735\n",
      "  30182 46339 36359 32252]]\n"
     ]
    }
   ],
   "source": [
    "# 随机生成矩阵并存储到相应的数据缓冲区中\n",
    "np.random.seed(3)  # 设置随机种子以确保生成的随机数相同\n",
    "input_matrix = np.random.randint(0, 100, size=(row, col), dtype=Input_DataType)\n",
    "weight_matrix = np.random.randint(0, 100, size=(col, col1), dtype=Weight_DataType)\n",
    "print(\"Randomly generated input buffer:\")\n",
    "print(input_matrix)\n",
    "print(\"\\nRandomly generated weight buffer:\")\n",
    "print(weight_matrix)\n",
    "\n",
    "# 执行打包操作\n",
    "pack_matrix_to_buffer(input_matrix, MATRIX_WIDTH, input_buffer)\n",
    "pack_matrix_to_buffer(weight_matrix, MATRIX_WIDTH, weight_buffer)\n",
    "print(\"Packed input buffer:\")\n",
    "print(input_buffer)\n",
    "print(\"Packed input buffer:\")\n",
    "print(weight_buffer)\n",
    "\n",
    "# 将输入矩阵转换为np.int32类型，以避免溢出\n",
    "input_matrix_int32 = input_matrix.astype(np.int32)\n",
    "weight_matrix_int32 = weight_matrix.astype(np.int32)\n",
    "# 定义input_buffer和weight_buffer的矩阵乘法结果的结果矩阵\n",
    "pt0 = time.perf_counter()\n",
    "result_matrix = np.dot(input_matrix_int32, weight_matrix_int32)\n",
    "pt1 = time.perf_counter()\n",
    "time_sw = pt1 - pt0\n",
    "print(\"pure software: %fs\" % time_sw)\n",
    "# 打印矩阵乘法结果\n",
    "print(\"Matrix multiplication result:\")\n",
    "print(result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "INFO - Blocked GEMM test: dim_I=16, dim_J=16, dim_K=16, block=8, bias_use=0\n",
      "compute_insn_count: 65\n",
      "insn_size: 130\n",
      "insn_idx: 90\n",
      "done: 2\n",
      "INFO - Saa run time: 0.000494s\n",
      "INFO - Synchronization time: 0.494095ms\n",
      "INFO - Throughput: 0.024870GOPs/s\n",
      "[[24987 29480 25807 28555 27965 32766 27694 13375 23178 19190 22626 30452\n",
      "  23071 30627 28060 22068]\n",
      " [34062 35328 40731 29885 20304 39995 27828 24688 25476 26291 25811 38416\n",
      "  26828 35212 33943 36255]\n",
      " [35518 42848 45164 34488 28638 45081 38295 22877 35412 31074 27533 38223\n",
      "  30317 48155 39227 34637]\n",
      " [38598 39159 42572 33418 33310 42301 37854 29117 32001 31627 36325 44065\n",
      "  28905 46872 37960 40761]\n",
      " [30109 33346 28580 29936 33739 37535 32295 21067 25283 24686 25860 33029\n",
      "  24698 37175 32950 29071]\n",
      " [39235 36115 40943 32164 29017 44383 35203 23209 31738 24161 32013 44321\n",
      "  31642 35284 34913 35455]\n",
      " [28696 28542 35681 30552 24798 31369 32104 18886 23136 22893 28179 28754\n",
      "  20099 35092 22651 26708]\n",
      " [38936 39875 47241 38118 24559 45479 35422 18750 26734 29830 38431 43551\n",
      "  35876 45817 37814 35370]\n",
      " [36179 38809 39842 35367 32706 41490 36688 21510 27618 30314 31611 39858\n",
      "  29854 44577 32527 33818]\n",
      " [47385 49312 46354 42449 44371 54911 48756 31569 37853 40762 45985 50622\n",
      "  39077 56845 49147 43080]\n",
      " [42438 49702 48309 42097 36417 52741 42628 30503 36957 38658 42275 50469\n",
      "  36738 58060 49462 41770]\n",
      " [42033 41578 45538 41775 32666 42954 42178 17257 28101 35663 38055 43721\n",
      "  36256 50396 35630 32844]\n",
      " [39898 35779 43212 28823 30472 39989 36905 19687 27147 33927 37932 45729\n",
      "  35908 43053 38845 36232]\n",
      " [41068 41256 44884 33904 34611 45725 41772 26352 35092 36004 38082 42994\n",
      "  35136 50259 41062 35399]\n",
      " [28692 31952 28437 28210 22730 31329 29470 15897 22696 24466 21858 26720\n",
      "  23120 38619 22976 22358]\n",
      " [34929 37646 40390 29465 28305 38145 37877 21862 32760 31982 30503 37735\n",
      "  30182 46339 36359 32252]]\n"
     ]
    }
   ],
   "source": [
    "# 执行分块矩阵乘法\n",
    "blocked_gemm_test(saa_driver,\n",
    "              row, \n",
    "              col1, \n",
    "              col, \n",
    "              input_buffer, \n",
    "              weight_buffer,\n",
    "              bias_buffer, \n",
    "              output_buffer, \n",
    "              block_size, \n",
    "              0)\n",
    "# 检查输出\n",
    "print(output_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 回收缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看完成后清空缓冲区\n",
    "del output_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct_buffer = allocate(shape = (1), cacheable = 0, dtype = Instruct_DataType)\n",
    "# instructions = [] #临时存储指令\n",
    "# # insn_test = getWeightPreloadComputeInsn(\n",
    "# #     1,\n",
    "# #     1,\n",
    "# #     1,\n",
    "# #     1,\n",
    "# #     1,\n",
    "# #     1)\n",
    "\n",
    "# # insn_test = getWeightPreloadInsn(1, 1)\n",
    "\n",
    "# # insn_test = get2DLoadStoreInsn(\n",
    "# #                           1, \n",
    "# #                           1, \n",
    "# #                           1, \n",
    "# #                           1, \n",
    "# #                           1, \n",
    "# #                           1, \n",
    "# #                           1)\n",
    "\n",
    "# insn_test = getComputeInsn(1, \n",
    "#                         1, \n",
    "#                         1, \n",
    "#                         1)\n",
    "    \n",
    "# instructions.append(insn_test)\n",
    "# # 将生成的指令批量存入缓冲区，然后启动saa让其读取指令\n",
    "# for i, instruction in enumerate(instructions):\n",
    "#     instruct_buffer[i] = np.frombuffer(instruction, dtype=Instruct_DataType)\n",
    "#     print(instruct_buffer[i])\n",
    "#     print_binary(instruct_buffer[i]) # 输出指令的二进制表示\n",
    "\n",
    "\n",
    "# from pynq import allocate\n",
    "# import time\n",
    "# import numpy as np\n",
    "# wait_cycles = 100000 # 定义一次最多等待周期为1000万周期\n",
    "# def blocked_gemm_test(saa_driver,\n",
    "#               dim_I, \n",
    "#               dim_J, \n",
    "#               dim_K, \n",
    "#               input, \n",
    "#               weight,\n",
    "#               bias, \n",
    "#               output, \n",
    "#               block, \n",
    "#               bias_use):\n",
    "    \n",
    "#     print(\"=====================================================================================\")\n",
    "#     print(f\"INFO - Blocked GEMM test: dim_I={dim_I}, dim_J={dim_J}, dim_K={dim_K}, block={block}, bias_use={bias_use}\")\n",
    "    \n",
    "#     # 计算分块\n",
    "#     dim_I_block = dim_I // MATRIX_WIDTH\n",
    "#     dim_J_block = dim_J // MATRIX_WIDTH\n",
    "#     dim_K_block = dim_K // MATRIX_WIDTH\n",
    "\n",
    "#     # 计算指令数量\n",
    "#     insn_load_size = (dim_I_block * dim_K_block) + (dim_J_block * dim_K_block)\n",
    "#     insn_compute_size = 2 * dim_I_block * dim_K_block * dim_J_block  # 不使用权重复用\n",
    "# #     insn_compute_size = (dim_I_block + 1) * dim_K_block * dim_J_block  # 使用权重复用\n",
    "# #     insn_compute_size = dim_I_block * dim_K_block * dim_J_block + 1  # 使用权重复用和双缓冲\n",
    "#     insn_store_size = dim_I_block * dim_J_block\n",
    "#     insn_size = insn_load_size + insn_store_size + insn_compute_size + 1\n",
    "\n",
    "#     # 初始化指令队列\n",
    "#     insn_buf = allocate(shape = (insn_size), cacheable = 0, dtype = Instruct_DataType)\n",
    "#     insn_idx = 0\n",
    "    \n",
    "#     # 生成加载Input指令\n",
    "#     for i in range(dim_I_block):\n",
    "#         for k in range(dim_K_block):\n",
    "#             buffer_start = 0\n",
    "#             dram_start = 0\n",
    "#             A_block = i*dim_K_block+k\n",
    "#             buffer_offset = buffer_start + A_block * MATRIX_WIDTH\n",
    "#             dram_offset = dram_start + i * dim_K_block * MATRIX_WIDTH * MATRIX_WIDTH + k * MATRIX_WIDTH\n",
    "#             insn_buf[insn_idx] = get2DLoadStoreInsn(OPCODE_LOAD, \n",
    "#                                       INPUT_BUFFER_ID, \n",
    "#                                       buffer_offset, \n",
    "#                                       dram_offset, \n",
    "#                                       MATRIX_WIDTH, \n",
    "#                                       MATRIX_WIDTH, \n",
    "#                                       dim_K)\n",
    "#             insn_idx += 1\n",
    "\n",
    "#     # 生成加载weight指令\n",
    "#     for k in range(dim_K_block):\n",
    "#         for j in range(dim_J_block):\n",
    "#             buffer_start = 0\n",
    "#             dram_start = 0\n",
    "#             A_block = k * dim_J_block + j\n",
    "#             buffer_offset = buffer_start + A_block * MATRIX_WIDTH\n",
    "#             dram_offset = dram_start + k * dim_J_block * MATRIX_WIDTH * MATRIX_WIDTH + j * MATRIX_WIDTH \n",
    "#             insn_buf[insn_idx] = get2DLoadStoreInsn(OPCODE_LOAD, \n",
    "#                                       WEIGHT_BUFFER_ID, \n",
    "#                                       buffer_offset, \n",
    "#                                       dram_offset, \n",
    "#                                       MATRIX_WIDTH, \n",
    "#                                       MATRIX_WIDTH, \n",
    "#                                       dim_J)\n",
    "#             insn_idx += 1\n",
    "    \n",
    "#     # 生成计算指令\n",
    "#     # 用于切换权重寄存器，最先使用 weight1\n",
    "#     pingpang = 0\n",
    "#     wb_start_addr = 0\n",
    "#     input_start_addr = 0\n",
    "#     output_start_addr = 0\n",
    "#     weight_offset = 0\n",
    "#     output_offset = 0\n",
    "#     input_offset = 0\n",
    "#     accumulate = 0\n",
    "    \n",
    "#     # 初始化指令计数\n",
    "#     compute_count = insn_idx\n",
    "\n",
    "#     # 迭代公共维度块和输出列块\n",
    "#     for k in range(dim_K_block):\n",
    "#         for j in range(dim_J_block):\n",
    "#             # 计算权重偏移\n",
    "#             weight_offset = wb_start_addr + (k * dim_J_block + j) * MATRIX_WIDTH\n",
    "#             accumulate = 0 if k == 0 else 1\n",
    "\n",
    "#             # 第一次加载权重，使用初始寄存器，无法双缓冲\n",
    "#             if k == 0 and j == 0:\n",
    "#                 insn_buf[insn_idx] = getWeightPreloadInsn(weight_offset, pingpang)\n",
    "#                 insn_idx += 1\n",
    "#             else:\n",
    "#                 # 剩下的权重加载可以进行双缓冲\n",
    "# #                 insn_buf[insn_idx] = getWeightPreloadComputeInsn(\n",
    "# #                     input_offset,\n",
    "# #                     weight_offset,\n",
    "# #                     output_offset,\n",
    "# #                     pingpang,\n",
    "# #                     pingpang,\n",
    "# #                     accumulate)\n",
    "# #                 insn_idx += 1\n",
    "    \n",
    "# #                 insn_buf[insn_idx] = getComputeInsn(input_offset, \n",
    "# #                                         output_offset, \n",
    "# #                                         pingpang, \n",
    "# #                                         accumulate)\n",
    "# #                 insn_idx += 1\n",
    "#                 insn_buf[insn_idx] = getComputeInsn(input_offset, \n",
    "#                                         output_offset, \n",
    "#                                         pingpang, \n",
    "#                                         accumulate)\n",
    "#                 insn_idx += 1\n",
    "#                 insn_buf[insn_idx] = getWeightPreloadInsn(weight_offset, not pingpang)\n",
    "#                 insn_idx += 1\n",
    "                \n",
    "#                 pingpang = not pingpang  # 切换加载寄存器和计算寄存器\n",
    "\n",
    "\n",
    "#             # 迭代输出行块\n",
    "#             for i in range(dim_I_block):\n",
    "#                 output_offset = output_start_addr + (i * dim_J_block + j) * MATRIX_WIDTH\n",
    "#                 input_offset = input_start_addr + (i * dim_K_block + k) * MATRIX_WIDTH\n",
    "\n",
    "#                 # 如果不是最后一个计算，使用 getComputeInsn 计算\n",
    "#                 if i != dim_I_block - 1:\n",
    "#                     insn_buf[insn_idx] = getComputeInsn(input_offset, \n",
    "#                                             output_offset, \n",
    "#                                             pingpang, \n",
    "#                                             accumulate)\n",
    "#                     insn_idx += 1\n",
    "#                 # 如果是最后一个权重块，使用当前寄存器进行计算\n",
    "#                 if i == dim_I_block - 1 and j == dim_J_block - 1 and k == dim_K_block - 1:\n",
    "#                     insn_buf[insn_idx] = getComputeInsn(input_offset, \n",
    "#                                             output_offset, \n",
    "#                                             pingpang, \n",
    "#                                             accumulate)\n",
    "#                     insn_idx += 1\n",
    "# #             if k != 0 and j != 0:\n",
    "# #             insn_buf[insn_idx] = getComputeInsn(input_offset, \n",
    "# #                                     output_offset, \n",
    "# #                                     pingpang, \n",
    "# #                                     accumulate)\n",
    "#             insn_idx += 1\n",
    "    \n",
    "    \n",
    "#     # 更新计算指令的数量\n",
    "#     compute_count = insn_idx - compute_count\n",
    "#     print(f\"compute_count: {compute_count}\")    \n",
    "\n",
    "    \n",
    "#     # 生成存储指令\n",
    "#     for i in range(dim_I_block):\n",
    "#         for j in range(dim_J_block):\n",
    "#             buffer_start = 0\n",
    "#             dram_start = 0\n",
    "#             A_block = i * dim_J_block + j\n",
    "#             buffer_offset = buffer_start + A_block * MATRIX_WIDTH\n",
    "#             dram_offset = dram_start + i * dim_J_block * MATRIX_WIDTH * MATRIX_WIDTH + j * MATRIX_WIDTH\n",
    "#             insn_buf[insn_idx] = get2DLoadStoreInsn(OPCODE_STORE, \n",
    "#                                        OUTPUT_BUFFER_ID, \n",
    "#                                        buffer_offset, \n",
    "#                                        dram_offset, \n",
    "#                                        MATRIX_WIDTH, \n",
    "#                                        MATRIX_WIDTH, \n",
    "#                                        dim_J)\n",
    "#             insn_idx += 1\n",
    "            \n",
    "#     # 生成结束指令\n",
    "#     insn_buf[insn_idx] = getFinishInsn()\n",
    "#     insn_idx += 1\n",
    "\n",
    "#     print(\"insn_size\",insn_size)\n",
    "#     print(\"insn_idx\",insn_idx)\n",
    "#     print(\"insn\",insn_idx)\n",
    "#     for i in range(insn_idx):\n",
    "#         if i>=insn_load_size and i<insn_load_size+compute_count:\n",
    "#             print_binary(insn_buf[i])\n",
    "            \n",
    "#     # 运行SAA硬件\n",
    "#     pt0 = time.perf_counter()\n",
    "#     saa_driver.run_saa(insn_idx,\n",
    "#            insn_buf.physical_address,\n",
    "#            input.physical_address,\n",
    "#            weight.physical_address,\n",
    "#            output.physical_address,\n",
    "#            wait_cycles)\n",
    "#     pt1 = time.perf_counter()\n",
    "#     time_sw = pt1 - pt0\n",
    "#     print(\"saa run time: %fs\" % time_sw)     \n",
    "    \n",
    "#     # 计算吞吐量\n",
    "    \n",
    "    \n",
    "#     return 0 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
